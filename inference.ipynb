{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "383e743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f697cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     model_name    = 'Unet'\n",
    "#     backbone      = 'efficientnet-b7'\n",
    "#     ckpt_path     = '/mnt/SSD/workspace/roads_buildings/src/exps/1700856032.515359/best_epoch.bin'\n",
    "#     img_size      = [1024, 1024]\n",
    "#     num_classes   = 1\n",
    "#     device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8e8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(backbone, num_classes, device):\n",
    "    model = smp.Unet(\n",
    "        encoder_name=backbone,      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=num_classes,        # model output channels (number of classes in your dataset)\n",
    "        activation=None,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(backbone, num_classes, device, path):\n",
    "    model = build_model(backbone, num_classes, device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee9b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_array_img(img):\n",
    "    img = img.astype('float32') # original is uint16\n",
    "    img = img / 255\n",
    "    # img = A.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx # scale image to [0, 1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38afb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, size=None):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)[:, :, :3]\n",
    "    init_shape = img.shape[:2]\n",
    "    if size:\n",
    "        img = cv2.resize(img, size)\n",
    "    img = img.astype('float32') # original is uint16\n",
    "    img = img / 255\n",
    "    # img = A.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx # scale image to [0, 1]\n",
    "    return img, init_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624fd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full(img_path):\n",
    "    threshold = 0.5\n",
    "#     img_path = '/mnt/SSD/workspace/roads_buildings/train/train/images/train_image_014.png'\n",
    "    image, shape = load_img(img_path, (1024, 1024))\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n",
    "    image = image.cuda()\n",
    "    with torch.no_grad():\n",
    "        pred = model(image)\n",
    "        pred = (nn.Sigmoid()(pred)>=threshold).double()\n",
    "        pred = pred.cpu().numpy().astype(np.uint8)[0][0]\n",
    "        pred = cv2.resize(pred, (shape[1], shape[0]), cv2.INTER_NEAREST)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8268217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_patched(model, image, patch_size, step):\n",
    "#     image = torch.nn.functional.pad(image, (step, step, step, step))\n",
    "    _, input_h, input_w = image.shape\n",
    "    \n",
    "    segm_img = torch.zeros((input_h, input_w), dtype=torch.float32)\n",
    "    patch_num=1\n",
    "    for i in range(0, input_h, step):   #Steps of 256\n",
    "        for j in range(0,input_w, step):  #Steps of 256\n",
    "            \n",
    "            input_image = torch.zeros((3, patch_size, patch_size))\n",
    "            \n",
    "            single_patch = image[:, i:i+patch_size, j:j+patch_size]\n",
    "#             single_patch_norm = np.expand_dims(normalize(np.array(single_patch), axis=1),2)\n",
    "            single_patch_shape = single_patch.shape[-2:]\n",
    "#             single_patch_input = np.expand_dims(single_patch, 0)\n",
    "#             print(single_patch_input.shape)\n",
    "            \n",
    "    \n",
    "            input_image[:, :single_patch_shape[0], :single_patch_shape[1]] = single_patch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                single_patch_prediction = model(input_image.cuda().unsqueeze(0))\n",
    "#                 single_patch_prediction = nn.Sigmoid()(single_patch_prediction)\n",
    "                single_patch_prediction = single_patch_prediction.cpu().numpy()\n",
    "            \n",
    "            result_image = single_patch_prediction[:, :, :single_patch_shape[0], :single_patch_shape[1]]\n",
    "            \n",
    "#             segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += cv2.resize(single_patch_prediction, single_patch_shape[::-1])\n",
    "#             single_patch_prediction = np.expand_dims(np.expand_dims(single_patch_prediction, 0), 0)\n",
    "            segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += result_image\n",
    "          \n",
    "            patch_num+=1\n",
    "#     return segm_img.numpy()[step:-step, step:-step]\n",
    "    return segm_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b4a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.VerticalFlip(),\n",
    "        tta.Rotate90(angles=[0, 90, 180, 270]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def do_tta(image):\n",
    "    masks = []\n",
    "    for transformer in transforms: # custom transforms or e.g. tta.aliases.d4_transform() \n",
    "\n",
    "        # augment image\n",
    "        augmented_image = transformer.augment_image(image.cuda())\n",
    "\n",
    "        # pass to model\n",
    "        with torch.no_grad():\n",
    "            model_output = model(augmented_image).detach().cpu()\n",
    "\n",
    "        # reverse augmentation for mask and label\n",
    "        deaug_mask = transformer.deaugment_mask(model_output)\n",
    "\n",
    "        # save results\n",
    "        masks.append(deaug_mask)\n",
    "\n",
    "    masks = torch.stack(masks)[:, 0, 0].sum(0)\n",
    "    return masks\n",
    "\n",
    "def prediction_patched_tta(model, image, patch_size, step):\n",
    "#     image = torch.nn.functional.pad(image, (step, step, step, step))\n",
    "    _, input_h, input_w = image.shape\n",
    "    \n",
    "    segm_img = torch.zeros((input_h, input_w), dtype=torch.float32)\n",
    "    patch_num=1\n",
    "    for i in range(0, input_h, step):   #Steps of 256\n",
    "        for j in range(0,input_w, step):  #Steps of 256\n",
    "            \n",
    "            input_image = torch.zeros((3, patch_size, patch_size))\n",
    "            \n",
    "            single_patch = image[:, i:i+patch_size, j:j+patch_size]\n",
    "#             single_patch_norm = np.expand_dims(normalize(np.array(single_patch), axis=1),2)\n",
    "            single_patch_shape = single_patch.shape[-2:]\n",
    "#             single_patch_input = np.expand_dims(single_patch, 0)\n",
    "#             print(single_patch_input.shape)\n",
    "            \n",
    "    \n",
    "            input_image[:, :single_patch_shape[0], :single_patch_shape[1]] = single_patch\n",
    "            single_patch_prediction = do_tta(input_image.unsqueeze(0))\n",
    "#             with torch.no_grad():\n",
    "#                 single_patch_prediction = model(input_image.cuda().unsqueeze(0))\n",
    "            single_patch_prediction = single_patch_prediction.cpu().unsqueeze(0).unsqueeze(0).numpy()\n",
    "            \n",
    "            result_image = single_patch_prediction[:, :, :single_patch_shape[0], :single_patch_shape[1]]\n",
    "            \n",
    "#             segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += cv2.resize(single_patch_prediction, single_patch_shape[::-1])\n",
    "#             single_patch_prediction = np.expand_dims(np.expand_dims(single_patch_prediction, 0), 0)\n",
    "            segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += result_image\n",
    "          \n",
    "            patch_num+=1\n",
    "#     return segm_img.numpy()[step:-step, step:-step]\n",
    "    return segm_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169e41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patched(img_path, step, use_tta=False):\n",
    "    image = torch.from_numpy(load_img(img_path, None)[0]).permute(2, 0, 1)\n",
    "    if use_tta:\n",
    "        mask_patched = prediction_patched_tta(model, image, patch_size=1024, step=step)\n",
    "    else:\n",
    "        mask_patched = prediction_patched(model, image, patch_size=1024, step=step)\n",
    "    return mask_patched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6659b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        'path': 'exps/1700856032.515359',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'comment': 'b7, imagenet',\n",
    "        'score': 0.6813924908638\n",
    "    },\n",
    "    {\n",
    "        'path': 'exps/1700845040.797431',\n",
    "        'backbone': 'timm-efficientnet-b7',\n",
    "        'comment': 'timm-b7, imagenet, opensource pretrained',\n",
    "        'score': 0.6732556819915771\n",
    "    },\n",
    "    {\n",
    "        'path': 'exps/1700823919.5695016',\n",
    "        'backbone': 'timm-efficientnet-b7',\n",
    "        'comment': 'timm-b7, noisy-student',\n",
    "        'score': 0.6628191471099854\n",
    "    },\n",
    "    {\n",
    "        'path': 'exps/1700884866.665121',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'comment': 'efficientnet-b7, our dataset',\n",
    "        'score': 0.6754\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09424c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '/mnt/SSD/workspace/roads_buildings/src/'\n",
    "model_idx = -1\n",
    "model = load_model(models[model_idx]['backbone'], 1, 'cuda', models_path+models[model_idx]['path']+'/best_epoch.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5948316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # использовать для инференса!!!\n",
    "# img_path = '/mnt/SSD/workspace/roads_buildings/train/train/images/train_image_006.png'\n",
    "# pred_patched = predict_patched(img_path, step=256, use_tta=False)\n",
    "# pred = (nn.Sigmoid()(pred_patched)>0.5).numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53869d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                         | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████████████████████▏                                                                                                                                                                                      | 1/8 [00:53<06:16, 53.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████████████████████████████████████▍                                                                                                                                  | 3/8 [03:01<04:47, 57.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                        | 4/8 [03:17<02:44, 41.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_007.png\n"
     ]
    }
   ],
   "source": [
    "for img_path in tqdm(glob.glob('/mnt/SSD/workspace/roads_buildings/test_dataset_test/images/*')):\n",
    "    pred_patched = predict_patched(img_path, step=1024, use_tta=True)\n",
    "    pred = (nn.Sigmoid()(pred_patched)>0.5).numpy().astype(np.uint8)\n",
    "    mask_path = img_path.replace('image', 'mask')\n",
    "    print(mask_path)\n",
    "    Image.fromarray(pred).save(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0c991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e774c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b14ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1474b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54476419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702af73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4817c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba24cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/mnt/SSD/workspace/roads_buildings/train/train/images/train_image_006.png'\n",
    "# pred_patched = predict_patched(img_path, step=1024, use_tta=True)\n",
    "# pred = (nn.Sigmoid()(pred_patched)>0.5).numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bef85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f61f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                         | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n",
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████████████████████                                                                                                                                                                                      | 1/8 [03:19<23:16, 199.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n",
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████████████████████████████████████                                                                                                                                                            | 2/8 [08:24<26:09, 261.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████████████████████████████████████████████████████████████                                                                                                                                  | 3/8 [10:09<15:49, 189.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_003.png\n",
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                        | 4/8 [11:00<09:00, 135.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_007.png\n",
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n",
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                              | 5/8 [17:29<11:20, 226.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                    | 6/8 [19:02<06:02, 181.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_002.png\n",
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n",
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 7/8 [22:10<03:03, 183.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7, imagenet 0.6813924908638\n",
      "timm-b7, imagenet, opensource pretrained 0.6732556819915771\n",
      "timm-b7, noisy-student 0.6628191471099854\n",
      "efficientnet-b7, our dataset 0.6754\n",
      "/mnt/SSD/workspace/roads_buildings/test_dataset_test/masks/test_mask_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [24:31<00:00, 183.97s/it]\n"
     ]
    }
   ],
   "source": [
    "test_files = tqdm(glob.glob('/mnt/SSD/workspace/roads_buildings/test_dataset_test/images/*'))\n",
    "for img_path in test_files:\n",
    "    masks = []\n",
    "    for  model_dict in models:\n",
    "        print(model_dict['comment'], model_dict['score'])\n",
    "        model = load_model(model_dict['backbone'], 1, 'cuda', models_path+model_dict['path']+'/best_epoch.bin')\n",
    "        mask_path = img_path.replace('image', 'mask')\n",
    "        pred_patched = predict_patched(img_path, step=256, use_tta=False)\n",
    "        masks.append(pred_patched)\n",
    "    \n",
    "    pred = torch.stack(masks).numpy().sum(0)\n",
    "    pred = (nn.Sigmoid()(torch.from_numpy(pred))>0.5).numpy().astype(np.uint8)\n",
    "    mask_path = img_path.replace('image', 'mask')\n",
    "    print(mask_path)\n",
    "    Image.fromarray(pred).save(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c27f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61b11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd01b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e6f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f9204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e888dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd33ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
